---
title: 【PGGAN】PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION
date: 2019-11-25 20:17:16
tags:
  - GANs
categories: 
  - Paper Reading
---

## 摘要
本文提出了一种用来训练生成对抗网络的新方法。主要的idea是渐进式地增加生成器和判别器的规模：首先，从一个很低的分辨率开始，逐步添加新的层使得模型在训练过程中能更好地完善细节。这种话做法既能加速训练，也能使训练更加稳定，并且能生成质量极高的图，如$1024 \times 1024$分辨率的CELEBA图像。同时，本文还提出了一种可以提高生成图像多样性的方法，该方法在无监督的CIFAR 10上能获得8.80的分数。此外，本文还给出了一些阻止判别器和生成器之间不健康竞争的实现细节。最后，本文给出了一种新的关于图像生成质量和多样性的评价指标。

## 介绍
GAN由判别器和生成器组成。其中，生成器用来生成样本。例如可以通过隐变量生成图像，并且这些图像的分布和训练数据中图像的分布是没有区别的。此外，还需要一个判别器来评估生成图像和真实图像间的差别。我们使用梯度来引导两个网络向正确的方向手收敛。显然，生成器是主要的关注对象，而判别器在生成器被训练好后可以被抛弃掉。
但是，这种设计策略是有潜在问题的。当我们衡量训练数据的分布和生成数据的分布之间的差异时，如果这两个分布没有充分重叠，梯度会或多或少地指向一个随机方向，也就是说很容易被区分出来。
生成高分辨率的图像是一件困难的事，由于高分辨率使得生成图和实际图能很容易地分辨出来，这会放大GAN训练中的梯度问题。同时，由于内存的限制，高分辨率使得必须要用更小的minibatch，从而影响训练的稳定性。本文亮点在于渐进式地同时增加生成器和判别器，从低分辨率的图开始，逐渐增加新的层来完善图像细节。这样做能加速训练，并提升在高分辨率图像中的稳定性。

## PROGRESSIVE GROWING OF GANS
如图1所示，这种渐进增长方式使得训练在初期时发现图像分布的一些大尺度的结构，然后再将注意力转移至越来越精细的尺度细节上，而不是像传统方式那样同时学习所有尺度。
![图1](/img/pggan1.jpg)
我们使用彼此互为镜像且始终同步增长的生成器和判别器网络。在训练过程中，两个网络的所有层都保持可训练状态。当新的层加入网络时，我们平滑地将其加入进去，如图2所示。这样就避免了对已经训练好的较小分辨率的层的突然冲击。
![图2](/img/pggan2.jpg)
这种渐进式训练有几个好处。训练早期，这些生成的小分辨率图像是十分稳定的，因为里面没有包含多少类别信息。随着分辨率的逐渐增加，相比于将一个隐变量映射成$1024 \times 1024$的图像，这种渐进式的做法难度要低一些。
渐进式训练的另一个好处是减少了训练时间，因为大部分迭代都是在一个比较低的分辨率上进行的。

## 使用minibatch标准差来提高多样性
GAN有仅捕获训练集的一个子集的多样性的趋势，Salimans等人提出用minibatch discrimination来解决。他们是在整个小批量中计算特征统计量，从而鼓励生成和训练的minibatch图像具有相似的统计量。具体做法是在判别器最后面加入一个minibatch层，用来学习一个大的张量，该张量将输入投影到一组统计信息中。在一个minibatch中，每个样本都会生成一组单独的统计信息，并和上一层的输出进行concat,这样判别器便可以在内部使用这些统计信息。我们大大简化了这种方法，同时也提高了多样性。

**Minibatch discrimination**
具体的，假设样本$x_i$在D网络中某一层的特征向量为$f(x_i) \in R^A$，然后将$f(x_i)$乘以一个张量$T \in R^{A \times B \times C}$得到张量$M \in R^B \times C$。然后对每个样本之间的$M$的行向量计算L1距离，得到$c_b(x_i,x_j) = e^{-||M_{ib}-M_{jb}|| \_{L1}} \in R$，然后将所有的$c_b(x_i,x_j)$相加得到$o(x_i)\_b$，最后将$B$个$o(x_i)\_b$并起来得到一个大小为$B$的向量$o(x_i)$。最后，将$o(x_i)$和$f(x_i)$合并成一个向量作为D网络下一层的输入。如图3所示。
$$o(x_i)\_b = \sum_{j=1}^{n}{c_b(x_i,x_j)} \in \mathbb{R}$$
$$o(x_i) = [o(x_i)\_1, o(x_i)\_2, ..., o(x_i)\_B] \in \mathbb{B}$$
$$o(X) \in \mathbb{R}^{n \times B}$$
![图3](/img/pggan3.jpg)

我们简化的方法既没有可学习的参数也没有新的超参数。我们首先计算了minibatch上每个特征在每个空间位置上的标准差。然后对所有特征和空间位置的估计值求平均，得到单个值。我们复制该值，并将其与所有空间位置concat，生成一个附加的特征图。这个层可以插入到判别器的任意位置中，但我们发现插入在判别器的最末端效果最好。我们用更丰富的统计数据进行了进一步实验，发现并不能达到更好的效果。

## 生成器和判别器中的normalization
生成器和判别器之间的不健康竞争会导致GAN容易造成信号幅度的增大。早期大多数解决方法是在生成器和判别器中采用Batch normalization的变体。这些归一化方法最初是用来消除协变量偏移的（covariate shift）。然而，我们在GAN中并没有发现这个问题。而且，我们认为GAN中的实际需求是限制信号幅度和竞争。因此我们使用由两种成分组成的不同方法来解决这些问题，这两种成分都不包含可学习的参数。

### 均衡的学习率
我们使用标准正态分布来初始化权重，然后在运行阶段放缩权重。具体地，我们使用$\hat{w}\_i = w_i / c$，其中$w_i$为权重，$c$为He初始化得到的上一层的归一化常数。这样动态调整权重的好处是比较微妙的，并且还与常用的自适应随机梯度下降方法中的尺度不变性有关，例如RMSProp和Adam。这些方法通过其估计的标准偏差对梯度更新进行归一化，从而使更新与参数的大小无关。这样，如果某些参数的动态范围大于其他参数，则调整时间会更长。这是现代初始化方式导致的一种情况，因此，学习率可能同时太大又太小。而我们的方法可确保所有权重的动态范围以及学习速度均相同。

### 生成器中的pixel-wise特征向量归一化
为了避免由于竞争而导致生成器和判别器中幅度失控的情况，我们在每个卷积层之后将每个像素中的特征向量标准化为生成器中的单位长度。采用局部响应归一化来做。